{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto TS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So till now, we tried different models like ARIMA, SARIMA, Prophet but those required some data preprocessing & the results were not that great. Also, we have a variety of models for Time Series Analysis. So how we try them all and get the best results, for that we have a Python library Auto TS.\n",
    "\n",
    "Auto TS can train the data on 20 built-in models like ARIMA, ETS, VECM, etc. with different hyperparameters. It can handle Univariate and Multi-Variate Data and takes care of missing values and outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from autots import AutoTS\n",
    "import lightgbm\n",
    "import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.getcwd()\n",
    "data = pd.read_csv(os.path.join(path, 'data.csv'),sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>Area</th>\n",
       "      <th>DOR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>UNNIKRISHNAN</td>\n",
       "      <td>M</td>\n",
       "      <td>17.0</td>\n",
       "      <td>CORPORATION</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>MURAMMIL</td>\n",
       "      <td>M</td>\n",
       "      <td>6.0</td>\n",
       "      <td>ANDOORKONAM</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RAJAMMA</td>\n",
       "      <td>F</td>\n",
       "      <td>50.0</td>\n",
       "      <td>BALARAMAPURAM</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ANJU</td>\n",
       "      <td>F</td>\n",
       "      <td>15.0</td>\n",
       "      <td>CORPORATION</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>THANKAMANI</td>\n",
       "      <td>F</td>\n",
       "      <td>43.0</td>\n",
       "      <td>POOZHANAD</td>\n",
       "      <td>2013-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Name Sex   Age           Area        DOR\n",
       "0  UNNIKRISHNAN   M  17.0    CORPORATION 2013-01-01\n",
       "1      MURAMMIL   M   6.0    ANDOORKONAM 2013-01-01\n",
       "2       RAJAMMA   F  50.0  BALARAMAPURAM 2013-01-01\n",
       "3          ANJU   F  15.0    CORPORATION 2013-01-01\n",
       "4    THANKAMANI   F  43.0      POOZHANAD 2013-01-01"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['DOR'] = pd.to_datetime(data['DOR'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['Date'] = data['DOR'].map(lambda x: x.strftime('%Y-%m'))\n",
    "sumry = pd.DataFrame(data.Date.value_counts().sort_index()).reset_index()\n",
    "sumry.columns = ['Date', 'Cnt']\n",
    "# sumry['Date'] = pd.to_datetime(sumry['MnthWse'], format='%Y-%m')\n",
    "# sumry['Date'] = sumry['MnthWse'].map(lambda x: x.strftime('%Y-%m'))\n",
    "sumry['Date'] = pd.to_datetime(sumry.Date)\n",
    "sumry = sumry.set_index(sumry.Date)\n",
    "sumry.drop('Date', axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cnt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Date</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2013-01-01</th>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-02-01</th>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-03-01</th>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-04-01</th>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2013-05-01</th>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Cnt\n",
       "Date           \n",
       "2013-01-01  224\n",
       "2013-02-01  217\n",
       "2013-03-01  238\n",
       "2013-04-01  324\n",
       "2013-05-01  456"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumry.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013-01-01</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013-02-01</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013-03-01</td>\n",
       "      <td>238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013-04-01</td>\n",
       "      <td>324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013-05-01</td>\n",
       "      <td>456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>2018-08-01</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>2018-09-01</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>2018-10-01</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>2018-11-01</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>2018-12-01</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>72 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Date  Count\n",
       "0   2013-01-01    224\n",
       "1   2013-02-01    217\n",
       "2   2013-03-01    238\n",
       "3   2013-04-01    324\n",
       "4   2013-05-01    456\n",
       "..         ...    ...\n",
       "67  2018-08-01     27\n",
       "68  2018-09-01     24\n",
       "69  2018-10-01     23\n",
       "70  2018-11-01     32\n",
       "71  2018-12-01     29\n",
       "\n",
       "[72 rows x 2 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sumry = sumry.reset_index()\n",
    "ts = sumry.rename(columns={'Date': 'Date', 'Cnt': 'Count'})\n",
    "ts['Date'] = ts['Date'].astype(str)\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_weighting = {\n",
    "    'smape_weighting' : 10,\n",
    "    'mae_weighting' : 1,\n",
    "    'rmse_weighting' : 5,\n",
    "    'containment_weighting' : 1,\n",
    "    'runtime_weighting' : 0,\n",
    "    'spl_weighting': 1,\n",
    "    'contour_weighting': 0,\n",
    "}\n",
    "\n",
    "mod = AutoTS(forecast_length=36,\n",
    "             frequency='infer',\n",
    "             ensemble='simple',\n",
    "             max_generations=5,\n",
    "             num_validations=3,\n",
    "             metric_weighting=metric_weighting,\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fitting the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inferred frequency is: MS\n",
      "Model Number: 1 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 2 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 3 with model ETS in Validation 0 \n",
      "Model Number: 4 with model VECM in Validation 0 \n",
      "Transformation method not known or improperly entered, returning untransformed df\n",
      "Template Eval Error: TypeError(\"fit() missing 1 required positional argument: 'df'\",) in model 4: VECM\n",
      "Model Number: 5 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 5: VECM\n",
      "Model Number: 6 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 7 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 8 with model LastValueNaive in Validation 0 \n",
      "Model Number: 9 with model LastValueNaive in Validation 0 \n",
      "Model Number: 10 with model RollingRegression in Validation 0 \n",
      "Template Eval Error: NullFrequencyError('Cannot shift with no freq',) in model 10: RollingRegression\n",
      "Model Number: 11 with model RollingRegression in Validation 0 \n",
      "Template Eval Error: NullFrequencyError('Cannot shift with no freq',) in model 11: RollingRegression\n",
      "Model Number: 12 with model RollingRegression in Validation 0 \n",
      "Model Number: 13 with model RollingRegression in Validation 0 \n",
      "Model Number: 14 with model RollingRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6415\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6395\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6362\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6341\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.6319\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6297\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6272\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6262\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6248\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6244\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6235\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6221\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6193\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6203\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6192\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6200\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6179\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6169\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.6168\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6139\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6165\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6128\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.6137\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6108\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6097\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6084\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6083\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6066\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.6051\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6047\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6086\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6043\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6026\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6022\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6001\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5995\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - ETA: 0s - loss: 0.563 - 0s 13ms/step - loss: 0.6039\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 0.5981\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.5984\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5969\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5925\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.5920\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5970\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5978\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5958\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5972\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.5906\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.5871\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.5841\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5841\n",
      "Model Number: 15 with model GLM in Validation 0 \n",
      "Model Number: 16 with model GLM in Validation 0 \n",
      "Model Number: 17 with model GLS in Validation 0 \n",
      "Model Number: 18 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 18: VAR\n",
      "Model Number: 19 with model RollingRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 2.8461\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 1.1321\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.9173\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6165\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.5327\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.1502\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2319\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.6195\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.5659\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7211\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.4946\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.5352\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.2597\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.5421\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4472\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6496\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4454\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1806\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2485\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.3782\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3012\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.2963\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3143\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.3539\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.2686\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.4364\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.3937\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.1963\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2299\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3998\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.3812\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3941\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.4378\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1316\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.6445\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1635\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.3541\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.3337\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.2597\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.2540\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2213\n",
      "Epoch 42/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 0.2706\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3091\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.3737\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 0.3527\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.4018\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.1855\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.3558\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.3724\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.2851\n",
      "Model Number: 20 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 21 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 22 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 23 with model LastValueNaive in Validation 0 \n",
      "Model Number: 24 with model GluonTS in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 24: GluonTS\n",
      "Model Number: 25 with model VAR in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 25: VAR\n",
      "Model Number: 26 with model RollingRegression in Validation 0 \n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\",) in model 26: RollingRegression\n",
      "Model Number: 27 with model GLM in Validation 0 \n",
      "Template Eval Error: ValueError('NaN, inf or invalid value detected in weights, estimation infeasible.',) in model 27: GLM\n",
      "Model Number: 28 with model LastValueNaive in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 29 with model FBProphet in Validation 0 \n",
      "Model Number: 30 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 31 with model ETS in Validation 0 \n",
      "Model Number: 32 with model GLS in Validation 0 \n",
      "Model Number: 33 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 33: GluonTS\n",
      "Model Number: 34 with model ARIMA in Validation 0 \n",
      "Template Eval Error: ValueError('Length mismatch: Expected axis has 37 elements, new values have 1 elements',) in model 34: ARIMA\n",
      "Model Number: 35 with model RollingRegression in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 36 with model LastValueNaive in Validation 0 \n",
      "Model Number: 37 with model ARIMA in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/base/tsa_model.py:162: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/base/tsa_model.py:162: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 38 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 39 with model AverageValueNaive in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 39: AverageValueNaive\n",
      "Model Number: 40 with model LastValueNaive in Validation 0 \n",
      "Model Number: 41 with model LastValueNaive in Validation 0 \n",
      "Model Number: 42 with model ETS in Validation 0 \n",
      "Template Eval Error: ValueError('endog must be strictly positive when usingmultiplicative trend or seasonal components.',) in model 42: ETS\n",
      "Model Number: 43 with model DatepartRegression in Validation 0 \n",
      "Model Number: 44 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 45 with model AverageValueNaive in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 46 with model FBProphet in Validation 0 \n",
      "Model Number: 47 with model ZeroesNaive in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 48 with model FBProphet in Validation 0 \n",
      "Model Number: 49 with model GLS in Validation 0 \n",
      "Model Number: 50 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 51 with model WindowRegression in Validation 0 \n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Model Number: 52 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 53 with model RollingRegression in Validation 0 \n",
      "Model Number: 54 with model DatepartRegression in Validation 0 \n",
      "Model Number: 55 with model GLS in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 55: GLS\n",
      "Model Number: 56 with model DatepartRegression in Validation 0 \n",
      "Model Number: 57 with model WindowRegression in Validation 0 \n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\",) in model 57: WindowRegression\n",
      "New Generation: 1\n",
      "Model Number: 58 with model LastValueNaive in Validation 0 \n",
      "Model Number: 59 with model RollingRegression in Validation 0 \n",
      "Model Number: 60 with model RollingRegression in Validation 0 \n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\",) in model 60: RollingRegression\n",
      "Model Number: 61 with model RollingRegression in Validation 0 \n",
      "Model Number: 62 with model RollingRegression in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 63 with model GLS in Validation 0 \n",
      "Model Number: 64 with model GLS in Validation 0 \n",
      "Model Number: 65 with model GLS in Validation 0 \n",
      "Model Number: 66 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 67 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 68 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 69 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 70 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 71 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 72 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 73 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 74 with model UnobservedComponents in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/statespace/structural.py:498: SpecificationWarning:\n",
      "\n",
      "Specified model does not contain a stochastic element; irregular component added.\n",
      "\n",
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/statespace/structural.py:498: SpecificationWarning:\n",
      "\n",
      "Specified model does not contain a stochastic element; irregular component added.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 75 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 76 with model DatepartRegression in Validation 0 \n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Model Number: 77 with model DatepartRegression in Validation 0 \n",
      "Model Number: 78 with model DatepartRegression in Validation 0 \n",
      "Model Number: 79 with model WindowRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "Template Eval Error: UnboundLocalError(\"local variable 'logs' referenced before assignment\",) in model 79: WindowRegression\n",
      "Model Number: 80 with model WindowRegression in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\",) in model 80: WindowRegression\n",
      "Model Number: 81 with model WindowRegression in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\",) in model 81: WindowRegression\n",
      "Model Number: 82 with model WindowRegression in Validation 0 \n",
      "Model Number: 83 with model GLM in Validation 0 \n",
      "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.',) in model 83: GLM\n",
      "Model Number: 84 with model GLM in Validation 0 \n",
      "Model Number: 85 with model GLM in Validation 0 \n",
      "Model Number: 86 with model GLM in Validation 0 \n",
      "Model Number: 87 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 88 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 89 with model ZeroesNaive in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 90 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 91 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 92 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 93 with model FBProphet in Validation 0 \n",
      "Model Number: 94 with model ETS in Validation 0 \n",
      "Model Number: 95 with model ETS in Validation 0 \n",
      "Model Number: 96 with model ETS in Validation 0 \n",
      "Model Number: 97 with model ETS in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/holtwinters.py:744: ConvergenceWarning:\n",
      "\n",
      "Optimization failed to converge. Check mle_retvals.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 98 with model ARIMA in Validation 0 \n",
      "Model Number: 99 with model ARIMA in Validation 0 \n",
      "Model Number: 100 with model ARIMA in Validation 0 \n",
      "Model Number: 101 with model ARIMA in Validation 0 \n",
      "Model Number: 102 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 102: VECM\n",
      "Model Number: 103 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 103: VECM\n",
      "Model Number: 104 with model VECM in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 104: VECM\n",
      "Model Number: 105 with model VECM in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 105: VECM\n",
      "Model Number: 106 with model VAR in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 106: VAR\n",
      "Model Number: 107 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 107: VAR\n",
      "Model Number: 108 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 108: VAR\n",
      "Model Number: 109 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 109: VAR\n",
      "Model Number: 110 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 110: GluonTS\n",
      "Model Number: 111 with model GluonTS in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 111: GluonTS\n",
      "Model Number: 112 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 112: GluonTS\n",
      "Model Number: 113 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 113: GluonTS\n",
      "New Generation: 2\n",
      "Model Number: 114 with model LastValueNaive in Validation 0 \n",
      "Model Number: 115 with model LastValueNaive in Validation 0 \n",
      "Model Number: 116 with model GLS in Validation 0 \n",
      "Model Number: 117 with model GLS in Validation 0 \n",
      "Model Number: 118 with model GLS in Validation 0 \n",
      "Model Number: 119 with model RollingRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: nan\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 15ms/step - loss: nan\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: nan\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: nan\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: nan\n",
      "Template Eval Error: ValueError('Model RollingRegression returned NaN for one or more series',) in model 119: RollingRegression\n",
      "Model Number: 120 with model RollingRegression in Validation 0 \n",
      "Model Number: 121 with model RollingRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 17ms/step - loss: nan\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: nan\n",
      "Epoch 25/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: nan\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 18ms/step - loss: nan\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: nan\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: nan\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 16ms/step - loss: nan\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 15ms/step - loss: nan\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: nan\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: nan\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: nan\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: nan\n",
      "Template Eval Error: ValueError('Model RollingRegression returned NaN for one or more series',) in model 121: RollingRegression\n",
      "Model Number: 122 with model RollingRegression in Validation 0 \n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\",) in model 122: RollingRegression\n",
      "Model Number: 123 with model GLM in Validation 0 \n",
      "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.',) in model 123: GLM\n",
      "Model Number: 124 with model GLM in Validation 0 \n",
      "Model Number: 125 with model GLM in Validation 0 \n",
      "Template Eval Error: ValueError('The first guess on the deviance function returned a nan.  This could be a boundary  problem and should be reported.',) in model 125: GLM\n",
      "Model Number: 126 with model GLM in Validation 0 \n",
      "Model Number: 127 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 128 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 129 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 130 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 131 with model WindowRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1359\n",
      "Epoch 2/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1340\n",
      "Epoch 3/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1326\n",
      "Epoch 4/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1322\n",
      "Epoch 5/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1286\n",
      "Epoch 6/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1284\n",
      "Epoch 7/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1297\n",
      "Epoch 8/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1262\n",
      "Epoch 9/50\n",
      "1/1 [==============================] - 0s 8ms/step - loss: 0.1262\n",
      "Epoch 10/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1267\n",
      "Epoch 11/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1248\n",
      "Epoch 12/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1221\n",
      "Epoch 13/50\n",
      "1/1 [==============================] - 0s 7ms/step - loss: 0.1227\n",
      "Epoch 14/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1204\n",
      "Epoch 15/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1198\n",
      "Epoch 16/50\n",
      "1/1 [==============================] - 0s 11ms/step - loss: 0.1194\n",
      "Epoch 17/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1196\n",
      "Epoch 18/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1183\n",
      "Epoch 19/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1173\n",
      "Epoch 20/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1155\n",
      "Epoch 21/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1144\n",
      "Epoch 22/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1138\n",
      "Epoch 23/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1135\n",
      "Epoch 24/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1123\n",
      "Epoch 25/50\n",
      "1/1 [==============================] - 0s 9ms/step - loss: 0.1114\n",
      "Epoch 26/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1108\n",
      "Epoch 27/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1123\n",
      "Epoch 28/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.1095\n",
      "Epoch 29/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1093\n",
      "Epoch 30/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1086\n",
      "Epoch 31/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1063\n",
      "Epoch 32/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1094\n",
      "Epoch 33/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1074\n",
      "Epoch 34/50\n",
      "1/1 [==============================] - 0s 6ms/step - loss: 0.1088\n",
      "Epoch 35/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1080\n",
      "Epoch 36/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1053\n",
      "Epoch 37/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1026\n",
      "Epoch 38/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1036\n",
      "Epoch 39/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1041\n",
      "Epoch 40/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.1023\n",
      "Epoch 41/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1039\n",
      "Epoch 42/50\n",
      "1/1 [==============================] - 0s 5ms/step - loss: 0.1018\n",
      "Epoch 43/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.1023\n",
      "Epoch 44/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.0995\n",
      "Epoch 45/50\n",
      "1/1 [==============================] - 0s 3ms/step - loss: 0.0987\n",
      "Epoch 46/50\n",
      "1/1 [==============================] - 0s 4ms/step - loss: 0.1001\n",
      "Epoch 47/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0975\n",
      "Epoch 48/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0991\n",
      "Epoch 49/50\n",
      "1/1 [==============================] - 0s 1ms/step - loss: 0.0967\n",
      "Epoch 50/50\n",
      "1/1 [==============================] - 0s 2ms/step - loss: 0.0994\n",
      "Model Number: 132 with model WindowRegression in Validation 0 \n",
      "Template Eval Error: ValueError('at least one array or dtype is required',) in model 132: WindowRegression\n",
      "Model Number: 133 with model WindowRegression in Validation 0 \n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\",) in model 133: WindowRegression\n",
      "Model Number: 134 with model WindowRegression in Validation 0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 135 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 136 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 137 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 138 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 139 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 140 with model UnobservedComponents in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/statespace/structural.py:498: SpecificationWarning:\n",
      "\n",
      "Specified model does not contain a stochastic element; irregular component added.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 141 with model DatepartRegression in Validation 0 \n",
      "Model Number: 142 with model DatepartRegression in Validation 0 \n",
      "Model Number: 143 with model DatepartRegression in Validation 0 \n",
      "Model Number: 144 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 145 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 146 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 147 with model ARIMA in Validation 0 \n",
      "Model Number: 148 with model ARIMA in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 149 with model ARIMA in Validation 0 \n",
      "Model Number: 150 with model ARIMA in Validation 0 \n",
      "Model Number: 151 with model ETS in Validation 0 \n",
      "Template Eval Error: ValueError('Can only dampen the trend component',) in model 151: ETS\n",
      "Model Number: 152 with model ETS in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/holtwinters.py:744: ConvergenceWarning:\n",
      "\n",
      "Optimization failed to converge. Check mle_retvals.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 153 with model ETS in Validation 0 \n",
      "Model Number: 154 with model ETS in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 155 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 156 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 157 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 158 with model FBProphet in Validation 0 \n",
      "Model Number: 159 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 159: VECM\n",
      "Model Number: 160 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 160: VECM\n",
      "Model Number: 161 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 161: VECM\n",
      "Model Number: 162 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 162: VECM\n",
      "Model Number: 163 with model VAR in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 163: VAR\n",
      "Model Number: 164 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 164: VAR\n",
      "Model Number: 165 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 165: VAR\n",
      "Model Number: 166 with model VAR in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 166: VAR\n",
      "Model Number: 167 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 167: GluonTS\n",
      "Model Number: 168 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 168: GluonTS\n",
      "Model Number: 169 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 169: GluonTS\n",
      "Model Number: 170 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 170: GluonTS\n",
      "New Generation: 3\n",
      "Model Number: 171 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 172 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 173 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 174 with model LastValueNaive in Validation 0 \n",
      "Model Number: 175 with model LastValueNaive in Validation 0 \n",
      "Model Number: 176 with model LastValueNaive in Validation 0 \n",
      "Model Number: 177 with model ETS in Validation 0 \n",
      "Template Eval Error: TypeError(\"int() argument must be a string, a bytes-like object or a number, not 'NoneType'\",) in model 177: ETS\n",
      "Model Number: 178 with model ETS in Validation 0 \n",
      "Model Number: 179 with model ETS in Validation 0 \n",
      "Model Number: 180 with model ETS in Validation 0 \n",
      "Template Eval Error: ValueError('Can only dampen the trend component',) in model 180: ETS\n",
      "Model Number: 181 with model GLS in Validation 0 \n",
      "Model Number: 182 with model GLS in Validation 0 \n",
      "Model Number: 183 with model GLS in Validation 0 \n",
      "Model Number: 184 with model RollingRegression in Validation 0 \n",
      "Model Number: 185 with model RollingRegression in Validation 0 \n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float32').\",) in model 185: RollingRegression\n",
      "Model Number: 186 with model RollingRegression in Validation 0 \n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\",) in model 186: RollingRegression\n",
      "Model Number: 187 with model RollingRegression in Validation 0 \n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\",) in model 187: RollingRegression\n",
      "Model Number: 188 with model GLM in Validation 0 \n",
      "Model Number: 189 with model GLM in Validation 0 \n",
      "Model Number: 190 with model GLM in Validation 0 \n",
      "Model Number: 191 with model GLM in Validation 0 \n",
      "Model Number: 192 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 193 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 194 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 195 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 196 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 197 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 198 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 199 with model WindowRegression in Validation 0 \n",
      "Template Eval Error: ValueError('at least one array or dtype is required',) in model 199: WindowRegression\n",
      "Model Number: 200 with model WindowRegression in Validation 0 \n",
      "[LightGBM] [Warning] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000023 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\",) in model 200: WindowRegression\n",
      "Model Number: 201 with model WindowRegression in Validation 0 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n",
      "[LightGBM] [Warning] Stopped training because there are no more leaves that meet the split requirements\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 202 with model WindowRegression in Validation 0 \n",
      "Model Number: 203 with model ARIMA in Validation 0 \n",
      "Model Number: 204 with model ARIMA in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/base/model.py:568: ConvergenceWarning:\n",
      "\n",
      "Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 205 with model ARIMA in Validation 0 \n",
      "Model Number: 206 with model ARIMA in Validation 0 \n",
      "Model Number: 207 with model DatepartRegression in Validation 0 \n",
      "[LightGBM] [Warning] There are no meaningful features, as all feature values are constant.\n",
      "Template Eval Error: LightGBMError('Check failed: (label) > (0) at /__w/1/s/python-package/compile/src/metric/regression_metric.hpp, line 274 .\\n',) in model 207: DatepartRegression\n",
      "Model Number: 208 with model DatepartRegression in Validation 0 \n",
      "Model Number: 209 with model DatepartRegression in Validation 0 \n",
      "Model Number: 210 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 211 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 212 with model ZeroesNaive in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 213 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 214 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 215 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 216 with model FBProphet in Validation 0 \n",
      "Model Number: 217 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 217: VECM\n",
      "Model Number: 218 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 218: VECM\n",
      "Model Number: 219 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 219: VECM\n",
      "Model Number: 220 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 220: VECM\n",
      "Model Number: 221 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 221: VAR\n",
      "Model Number: 222 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 222: VAR\n",
      "Model Number: 223 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 223: VAR\n",
      "Model Number: 224 with model VAR in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 224: VAR\n",
      "Model Number: 225 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 225: GluonTS\n",
      "Model Number: 226 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 226: GluonTS\n",
      "Model Number: 227 with model GluonTS in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 227: GluonTS\n",
      "Model Number: 228 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 228: GluonTS\n",
      "New Generation: 4\n",
      "Model Number: 229 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 230 with model UnobservedComponents in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/statespace/structural.py:498: SpecificationWarning:\n",
      "\n",
      "Specified model does not contain a stochastic element; irregular component added.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 231 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 232 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 233 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 234 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 235 with model SeasonalNaive in Validation 0 \n",
      "Template Eval Error: ValueError('Length mismatch: Expected axis has 37 elements, new values have 1 elements',) in model 235: SeasonalNaive\n",
      "Model Number: 236 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 237 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 238 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 239 with model FBProphet in Validation 0 \n",
      "Model Number: 240 with model WindowRegression in Validation 0 \n",
      "Template Eval Error: ValueError('at least one array or dtype is required',) in model 240: WindowRegression\n",
      "Model Number: 241 with model WindowRegression in Validation 0 \n",
      "Template Eval Error: ValueError('at least one array or dtype is required',) in model 241: WindowRegression\n",
      "Model Number: 242 with model WindowRegression in Validation 0 \n",
      "Model Number: 243 with model WindowRegression in Validation 0 \n",
      "Template Eval Error: ValueError('at least one array or dtype is required',) in model 243: WindowRegression\n",
      "Model Number: 244 with model LastValueNaive in Validation 0 \n",
      "Model Number: 245 with model LastValueNaive in Validation 0 \n",
      "Model Number: 246 with model LastValueNaive in Validation 0 \n",
      "Model Number: 247 with model ETS in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/holtwinters.py:744: ConvergenceWarning:\n",
      "\n",
      "Optimization failed to converge. Check mle_retvals.\n",
      "\n",
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/holtwinters.py:744: ConvergenceWarning:\n",
      "\n",
      "Optimization failed to converge. Check mle_retvals.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 248 with model ETS in Validation 0 \n",
      "Model Number: 249 with model ETS in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/holtwinters.py:744: ConvergenceWarning:\n",
      "\n",
      "Optimization failed to converge. Check mle_retvals.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 250 with model ETS in Validation 0 \n",
      "Template Eval Error: ValueError('Can only dampen the trend component',) in model 250: ETS\n",
      "Model Number: 251 with model ARIMA in Validation 0 \n",
      "Model Number: 252 with model ARIMA in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/base/tsa_model.py:162: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/base/tsa_model.py:162: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 253 with model ARIMA in Validation 0 \n",
      "Model Number: 254 with model ARIMA in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/base/tsa_model.py:162: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n",
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/base/tsa_model.py:162: ValueWarning:\n",
      "\n",
      "No frequency information was provided, so inferred frequency MS will be used.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 255 with model GLS in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 255: GLS\n",
      "Model Number: 256 with model GLS in Validation 0 \n",
      "Model Number: 257 with model GLS in Validation 0 \n",
      "Model Number: 258 with model GLM in Validation 0 \n",
      "Model Number: 259 with model GLM in Validation 0 \n",
      "Model Number: 260 with model GLM in Validation 0 \n",
      "Model Number: 261 with model GLM in Validation 0 \n",
      "Model Number: 262 with model RollingRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 119.0144\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 106.4191\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 98.3194\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 93.0511\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 88.3317\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 85.2969\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 83.9418\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 81.8350\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 79.0826\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 75.6020\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 71.6577\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 68.4655\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 65.7582\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 64.1221\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 61.8060\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 59.6937\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 58.7599\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 58.6753\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 56.9313\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 54.7933\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 53.4511\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 52.7110\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 52.3340\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 52.3618\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 50.8946\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 48.8534\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 47.0368\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 46.9402\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 48.3206\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 47.5404\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 44.9135\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - ETA: 0s - loss: 43.23 - 0s 8ms/step - loss: 41.3375\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 39.5370\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 39.3466\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 38.1128\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 36.1127\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 35.7285\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 37.3891\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 39.7000\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 39.3407\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 37.0491\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 35.3993\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 34.4738\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 33.7814\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 33.6829\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 33.8136\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 33.2853\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 33.5940\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 36.3256\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 38.2139\n",
      "Model Number: 263 with model RollingRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8373\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8362\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.8340\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8335\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.8311\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.8293\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.8285\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8276\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.8275\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 0.8266\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.8238\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.8213\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.8213\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.8161\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.8185\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 0.8147\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.8100\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.8056\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8019\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.8051\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.8012\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7924\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7934\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.7836\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7784\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.7743\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7708\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.7639\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7514\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 0.7513\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7481\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 0.7439\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7260\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.7239\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.7097\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 0.7045\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6925\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6885\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6641\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 0.6665\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.6523\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 0.6497\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 0.6412\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6168\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 0.6417\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6172\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6139\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 0.6097\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5814\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 0.5845\n",
      "Model Number: 264 with model RollingRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 258.0741\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 258.0590\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 258.0452\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 258.0332\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 258.0220\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 258.0060\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 257.9902\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 257.9765\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 257.9619\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 257.9444\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 257.9259\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 257.8959\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 257.8890\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 257.8611\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 257.8346\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 257.8077\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 257.7674\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 257.7438\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 257.6880\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 257.6707\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 257.6229\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 257.5668\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 257.5122\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 257.4676\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 257.3606\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 257.2873\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 257.2785\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 257.1352\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 257.0603\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 256.9237\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 256.9339\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 256.8206\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 256.5545\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 256.5139\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 256.3355\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 256.2720\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 20ms/step - loss: 255.9770\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 255.6163\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 255.6215\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 255.5485\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 19ms/step - loss: 255.2903\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 255.0958\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 254.8350\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 254.6031\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 254.3876\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 253.9563\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 253.9841\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 253.7060\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 253.3968\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 253.3234\n",
      "Model Number: 265 with model RollingRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 175.3038\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 175.2922\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 175.2791\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 175.2682\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 175.2557\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 175.2440\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 175.2301\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 175.2168\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - ETA: 0s - loss: 181.159 - 0s 13ms/step - loss: 175.2030\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 175.1900\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 175.1760\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 175.1543\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 175.1446\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 175.1225\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 175.1053\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 175.0886\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 175.0553\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 175.0318\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 174.9927\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 174.9848\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 174.9572\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 174.8986\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 174.8524\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 174.8153\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 15ms/step - loss: 174.7696\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 174.6935\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 174.6827\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 174.6019\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 174.5250\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 174.4251\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 174.4111\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 174.2914\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 174.1333\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 174.0481\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 173.9180\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 173.8696\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 173.6542\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 173.3817\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 173.3566\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 173.2391\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 172.9655\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 172.8315\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 172.5586\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 172.3691\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 172.1943\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 171.8741\n",
      "Epoch 47/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 9ms/step - loss: 171.9086\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 171.6354\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 171.2139\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 171.1738\n",
      "Model Number: 266 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 267 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 268 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 269 with model DatepartRegression in Validation 0 \n",
      "Model Number: 270 with model DatepartRegression in Validation 0 \n",
      "Model Number: 271 with model DatepartRegression in Validation 0 \n",
      "Model Number: 272 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 273 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 274 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 275 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 275: VECM\n",
      "Model Number: 276 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 276: VECM\n",
      "Model Number: 277 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 277: VECM\n",
      "Model Number: 278 with model VECM in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 278: VECM\n",
      "Model Number: 279 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 279: VAR\n",
      "Model Number: 280 with model VAR in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 280: VAR\n",
      "Model Number: 281 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 281: VAR\n",
      "Model Number: 282 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 282: VAR\n",
      "Model Number: 283 with model GluonTS in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 283: GluonTS\n",
      "Model Number: 284 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 284: GluonTS\n",
      "Model Number: 285 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 285: GluonTS\n",
      "Model Number: 286 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 286: GluonTS\n",
      "New Generation: 5\n",
      "Model Number: 287 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 288 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 289 with model UnobservedComponents in Validation 0 \n",
      "Model Number: 290 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 291 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 292 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 293 with model SeasonalNaive in Validation 0 \n",
      "Model Number: 294 with model RollingRegression in Validation 0 \n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\",) in model 294: RollingRegression\n",
      "Model Number: 295 with model RollingRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 60122.5664\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 60118.6484\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 60114.6172\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 60112.1016\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 60107.8477\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 60102.9414\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 60100.4688\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60097.0820\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 60092.1953\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60089.3359\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 60085.1914\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 60078.6094\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 60074.0156\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 60070.1562\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 60064.3320\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 60057.6016\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 60049.5938\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 60046.5312\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 60037.1016\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 60035.2969\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 60029.4062\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 60017.8203\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 60009.1797\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 59999.1211\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 59986.5156\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 59978.9766\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 59964.7461\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 59966.7969\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 59938.0430\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 59927.3359\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 59920.8555\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59903.7852\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 59871.4180\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 59839.9688\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 59831.1641\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 59800.3906\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 59783.3594\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 59719.3516\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 59693.1133\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 59678.5312\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 59613.8984\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 59623.7539\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 59548.2031\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 59482.4453\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 59495.5508\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 59408.0938\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 59401.8438\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 16ms/step - loss: 59317.9102\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 59223.6992\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 59155.7070\n",
      "Model Number: 296 with model RollingRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 114.6779\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 102.0673\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 94.1705\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 88.3530\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 83.4588\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 80.9740\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 79.2130\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 76.6420\n",
      "Epoch 9/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 5ms/step - loss: 73.4389\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 69.6167\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 65.5382\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 61.3392\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 57.9472\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 55.6856\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 54.7685\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 55.3159\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 53.9580\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 51.0081\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 48.4972\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 46.7991\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 45.8312\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 44.0512\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 43.7240\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 43.6806\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 41.4698\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 39.6596\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 40.1297\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 40.4466\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 39.4554\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 38.7929\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 38.3598\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 38.5530\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 37.7740\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 35.4982\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 34.8466\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 35.8126\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 36.5388\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 36.9714\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 38.1695\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 38.2203\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 36.2913\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 34.0738\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 33.3105\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 32.9552\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 32.5670\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 31.8766\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 31.4393\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 31.2355\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 30.3569\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 29.4483\n",
      "Model Number: 297 with model RollingRegression in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 114.6779\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 102.0673\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 94.1705\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 88.3530\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 83.4588\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 80.9740\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 79.2130\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 76.6420\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 73.4389\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 69.6167\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 65.5382\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 61.3392\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 57.9472\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 55.6856\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 54.7685\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 55.3159\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 53.9580\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 51.0081\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 48.4972\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 46.7991\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 45.8312\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 13ms/step - loss: 44.0512\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 43.7240\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 43.6806\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 41.4698\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 39.6596\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 40.1297\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 40.4466\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 39.4554\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 38.7929\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 38.3598\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 38.5530\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 37.7740\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 35.4982\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 34.8466\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 35.8126\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 36.5388\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 36.9714\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 38.1695\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 38.2203\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 36.2913\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 34.0738\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 33.3105\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 32.9552\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 32.5670\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 31.8766\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 31.4393\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 31.2355\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 30.3569\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 29.4483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 298 with model FBProphet in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:fbprophet:Disabling weekly seasonality. Run prophet with weekly_seasonality=True to override this.\n",
      "INFO:fbprophet:Disabling daily seasonality. Run prophet with daily_seasonality=True to override this.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 299 with model FBProphet in Validation 0 \n",
      "Model Number: 300 with model FBProphet in Validation 0 \n",
      "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 36',) in model 300: FBProphet\n",
      "Model Number: 301 with model FBProphet in Validation 0 \n",
      "Template Eval Error: ValueError('Unable to coerce to Series, length must be 1: given 36',) in model 301: FBProphet\n",
      "Model Number: 302 with model WindowRegression in Validation 0 \n",
      "Template Eval Error: ValueError('at least one array or dtype is required',) in model 302: WindowRegression\n",
      "Model Number: 303 with model WindowRegression in Validation 0 \n",
      "Model Number: 304 with model WindowRegression in Validation 0 \n",
      "Template Eval Error: ValueError(\"Input contains NaN, infinity or a value too large for dtype('float64').\",) in model 304: WindowRegression\n",
      "Model Number: 305 with model WindowRegression in Validation 0 \n",
      "Template Eval Error: ValueError('at least one array or dtype is required',) in model 305: WindowRegression\n",
      "Model Number: 306 with model LastValueNaive in Validation 0 \n",
      "Model Number: 307 with model LastValueNaive in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 307: LastValueNaive\n",
      "Model Number: 308 with model LastValueNaive in Validation 0 \n",
      "Model Number: 309 with model ETS in Validation 0 \n",
      "Model Number: 310 with model ETS in Validation 0 \n",
      "Model Number: 311 with model ETS in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/holtwinters.py:744: ConvergenceWarning:\n",
      "\n",
      "Optimization failed to converge. Check mle_retvals.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 312 with model ETS in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/prodigalson/.virtualenv/ts/lib/python3.6/site-packages/statsmodels/tsa/holtwinters.py:744: ConvergenceWarning:\n",
      "\n",
      "Optimization failed to converge. Check mle_retvals.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 313 with model ARIMA in Validation 0 \n",
      "Model Number: 314 with model ARIMA in Validation 0 \n",
      "Model Number: 315 with model ARIMA in Validation 0 \n",
      "Model Number: 316 with model ARIMA in Validation 0 \n",
      "Model Number: 317 with model AverageValueNaive in Validation 0 \n",
      "Model Number: 318 with model AverageValueNaive in Validation 0 \n",
      "Template Eval Error: ValueError('Length mismatch: Expected axis has 37 elements, new values have 1 elements',) in model 318: AverageValueNaive\n",
      "Model Number: 319 with model AverageValueNaive in Validation 0 \n",
      "Template Eval Error: ValueError('Length mismatch: Expected axis has 37 elements, new values have 1 elements',) in model 319: AverageValueNaive\n",
      "Model Number: 320 with model GLS in Validation 0 \n",
      "Model Number: 321 with model GLS in Validation 0 \n",
      "Model Number: 322 with model GLS in Validation 0 \n",
      "Model Number: 323 with model GLM in Validation 0 \n",
      "Model Number: 324 with model GLM in Validation 0 \n",
      "Model Number: 325 with model GLM in Validation 0 \n",
      "Model Number: 326 with model GLM in Validation 0 \n",
      "Model Number: 327 with model DatepartRegression in Validation 0 \n",
      "Model Number: 328 with model DatepartRegression in Validation 0 \n",
      "[LibLinear][LibLinear][LibLinear][LibLinear][LibLinear][LibLinear]Model Number: 329 with model DatepartRegression in Validation 0 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    1.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1000 out of 1000 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Number: 330 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 331 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 332 with model ZeroesNaive in Validation 0 \n",
      "Model Number: 333 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 333: VECM\n",
      "Model Number: 334 with model VECM in Validation 0 \n",
      "Transformation method not known or improperly entered, returning untransformed df\n",
      "Template Eval Error: TypeError(\"fit() missing 1 required positional argument: 'df'\",) in model 334: VECM\n",
      "Model Number: 335 with model VECM in Validation 0 \n",
      "Transformation method not known or improperly entered, returning untransformed df\n",
      "Template Eval Error: TypeError(\"fit() missing 1 required positional argument: 'df'\",) in model 335: VECM\n",
      "Model Number: 336 with model VECM in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VECM',) in model 336: VECM\n",
      "Model Number: 337 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 337: VAR\n",
      "Model Number: 338 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 338: VAR\n",
      "Model Number: 339 with model VAR in Validation 0 \n",
      "Template Eval Error: ValueError('Only gave one variable to VAR',) in model 339: VAR\n",
      "Model Number: 340 with model VAR in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 340: VAR\n",
      "Model Number: 341 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 341: GluonTS\n",
      "Model Number: 342 with model GluonTS in Validation 0 \n",
      "Template Eval Error: UnboundLocalError(\"local variable 'grouping_ids' referenced before assignment\",) in model 342: GluonTS\n",
      "Model Number: 343 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 343: GluonTS\n",
      "Model Number: 344 with model GluonTS in Validation 0 \n",
      "Template Eval Error: NameError(\"name 'ListDataset' is not defined\",) in model 344: GluonTS\n",
      "Model Number: 345 with model Ensemble in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 119.0144\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 106.4191\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 98.3194\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 93.0511\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 88.3317\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 85.2969\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 83.9418\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 81.8350\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 79.0826\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 75.6020\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 71.6577\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 68.4655\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 65.7582\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 64.1221\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 61.8060\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 59.6937\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 58.7599\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 58.6753\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 56.9313\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 54.7933\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 53.4511\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 52.7110\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 52.3340\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 52.3618\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 50.8946\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 48.8534\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 47.0368\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 46.9402\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 48.3206\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 47.5404\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 44.9135\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 41.3375\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 39.5370\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 39.3466\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 38.1128\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 36.1127\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 35.7285\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 37.3891\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 39.7000\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 39.3407\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 37.0491\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 35.3993\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 34.4738\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 33.7814\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 33.6829\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 33.8136\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 33.2853\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 33.5940\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 36.3256\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 38.2139\n",
      "Model Number: 346 with model Ensemble in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 119.0144\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 106.4191\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 98.3194\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 93.0511\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 88.3317\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 85.2969\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 83.9418\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 81.8350\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 79.0826\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 75.6020\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 71.6577\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 68.4655\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 65.7582\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 64.1221\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 61.8060\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 59.6937\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 58.7599\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 58.6753\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 56.9313\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 54.7933\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 53.4511\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 52.7110\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 52.3340\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 52.3618\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 50.8946\n",
      "Epoch 26/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2/2 [==============================] - 0s 7ms/step - loss: 48.8534\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 47.0368\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 46.9402\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 48.3206\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 47.5404\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 44.9135\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 41.3375\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 39.5370\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 39.3466\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 38.1128\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 36.1127\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 35.7285\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 37.3891\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 39.7000\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 39.3407\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 37.0491\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 35.3993\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 34.4738\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 33.7814\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 33.6829\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 33.8136\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 33.2853\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 33.5940\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 36.3256\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 38.2139\n",
      "Model Number: 347 with model Ensemble in Validation 0 \n",
      "Epoch 1/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 119.0144\n",
      "Epoch 2/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 106.4191\n",
      "Epoch 3/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 98.3194\n",
      "Epoch 4/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 93.0511\n",
      "Epoch 5/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 88.3317\n",
      "Epoch 6/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 85.2969\n",
      "Epoch 7/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 83.9418\n",
      "Epoch 8/50\n",
      "2/2 [==============================] - 0s 2ms/step - loss: 81.8350\n",
      "Epoch 9/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 79.0826\n",
      "Epoch 10/50\n",
      "2/2 [==============================] - 0s 18ms/step - loss: 75.6020\n",
      "Epoch 11/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 71.6577\n",
      "Epoch 12/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 68.4655\n",
      "Epoch 13/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 65.7582\n",
      "Epoch 14/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 64.1221\n",
      "Epoch 15/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 61.8060\n",
      "Epoch 16/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 59.6937\n",
      "Epoch 17/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 58.7599\n",
      "Epoch 18/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 58.6753\n",
      "Epoch 19/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 56.9313\n",
      "Epoch 20/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 54.7933\n",
      "Epoch 21/50\n",
      "2/2 [==============================] - 0s 14ms/step - loss: 53.4511\n",
      "Epoch 22/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 52.7110\n",
      "Epoch 23/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 52.3340\n",
      "Epoch 24/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 52.3618\n",
      "Epoch 25/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 50.8946\n",
      "Epoch 26/50\n",
      "2/2 [==============================] - 0s 6ms/step - loss: 48.8534\n",
      "Epoch 27/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 47.0368\n",
      "Epoch 28/50\n",
      "2/2 [==============================] - 0s 12ms/step - loss: 46.9402\n",
      "Epoch 29/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 48.3206\n",
      "Epoch 30/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 47.5404\n",
      "Epoch 31/50\n",
      "2/2 [==============================] - 0s 7ms/step - loss: 44.9135\n",
      "Epoch 32/50\n",
      "2/2 [==============================] - 0s 10ms/step - loss: 41.3375\n",
      "Epoch 33/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 39.5370\n",
      "Epoch 34/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 39.3466\n",
      "Epoch 35/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 38.1128\n",
      "Epoch 36/50\n",
      "2/2 [==============================] - 0s 9ms/step - loss: 36.1127\n",
      "Epoch 37/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 35.7285\n",
      "Epoch 38/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 37.3891\n",
      "Epoch 39/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 39.7000\n",
      "Epoch 40/50\n",
      "2/2 [==============================] - 0s 11ms/step - loss: 39.3407\n",
      "Epoch 41/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 37.0491\n",
      "Epoch 42/50\n",
      "2/2 [==============================] - 0s 4ms/step - loss: 35.3993\n",
      "Epoch 43/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 34.4738\n",
      "Epoch 44/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 33.7814\n",
      "Epoch 45/50\n",
      "2/2 [==============================] - 0s 3ms/step - loss: 33.6829\n",
      "Epoch 46/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 33.8136\n",
      "Epoch 47/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 33.2853\n",
      "Epoch 48/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 33.5940\n",
      "Epoch 49/50\n",
      "2/2 [==============================] - 0s 8ms/step - loss: 36.3256\n",
      "Epoch 50/50\n",
      "2/2 [==============================] - 0s 5ms/step - loss: 38.2139\n",
      "Too many training validations for length of data provided, decreasing num_validations to 0\n"
     ]
    }
   ],
   "source": [
    "mod = mod.fit(ts, date_col='Date', value_col='Count', id_col=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the details of the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Initiated AutoTS object with best model: \n",
       "UnobservedComponents\n",
       "{\"outlier_method\": \"clip\", \"outlier_threshold\": 3, \"outlier_position\": \"first\", \"fillna\": \"rolling mean\", \"transformation\": \"PowerTransformer\", \"second_transformation\": null, \"transformation_param\": null, \"detrend\": null, \"third_transformation\": null, \"transformation_param2\": null, \"fourth_transformation\": null, \"discretization\": null, \"n_bins\": null, \"coerce_integer\": false, \"context_slicer\": null}\n",
       "{\"level\": false, \"trend\": false, \"cycle\": true, \"damped_cycle\": false, \"irregular\": true, \"stochastic_trend\": true, \"stochastic_level\": false, \"stochastic_cycle\": false, \"regression_type\": \"Holiday\"}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = mod.predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = prediction.forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_results = mod.results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation = mod.results(\"validation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2019-01-01 00:00:00</th>\n",
       "      <td>17.117922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-02-01 00:00:00</th>\n",
       "      <td>47.123016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-03-01 00:00:00</th>\n",
       "      <td>50.575740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-04-01 00:00:00</th>\n",
       "      <td>55.366928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-05-01 00:00:00</th>\n",
       "      <td>61.704541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-06-01 00:00:00</th>\n",
       "      <td>69.847159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-07-01 00:00:00</th>\n",
       "      <td>80.098251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-08-01 00:00:00</th>\n",
       "      <td>92.792788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-09-01 00:00:00</th>\n",
       "      <td>108.272558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-10-01 00:00:00</th>\n",
       "      <td>126.846673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-11-01 00:00:00</th>\n",
       "      <td>148.734949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01 00:00:00</th>\n",
       "      <td>173.994782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-01-01 00:00:00</th>\n",
       "      <td>202.436977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-02-01 00:00:00</th>\n",
       "      <td>233.542496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-03-01 00:00:00</th>\n",
       "      <td>266.398756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-04-01 00:00:00</th>\n",
       "      <td>299.678577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-05-01 00:00:00</th>\n",
       "      <td>331.683993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-06-01 00:00:00</th>\n",
       "      <td>360.468434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-07-01 00:00:00</th>\n",
       "      <td>384.034097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-01 00:00:00</th>\n",
       "      <td>400.579854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-09-01 00:00:00</th>\n",
       "      <td>408.755527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-10-01 00:00:00</th>\n",
       "      <td>407.868580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-11-01 00:00:00</th>\n",
       "      <td>397.994945</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-12-01 00:00:00</th>\n",
       "      <td>379.966718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-01-01 00:00:00</th>\n",
       "      <td>141.076977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-02-01 00:00:00</th>\n",
       "      <td>325.673812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-03-01 00:00:00</th>\n",
       "      <td>293.274932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-04-01 00:00:00</th>\n",
       "      <td>259.954867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-05-01 00:00:00</th>\n",
       "      <td>227.346793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-06-01 00:00:00</th>\n",
       "      <td>196.698725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-07-01 00:00:00</th>\n",
       "      <td>168.843713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-08-01 00:00:00</th>\n",
       "      <td>144.231020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-09-01 00:00:00</th>\n",
       "      <td>122.995327</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-10-01 00:00:00</th>\n",
       "      <td>105.041423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-11-01 00:00:00</th>\n",
       "      <td>90.126960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2021-12-01 00:00:00</th>\n",
       "      <td>77.932622</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Count\n",
       "2019-01-01 00:00:00   17.117922\n",
       "2019-02-01 00:00:00   47.123016\n",
       "2019-03-01 00:00:00   50.575740\n",
       "2019-04-01 00:00:00   55.366928\n",
       "2019-05-01 00:00:00   61.704541\n",
       "2019-06-01 00:00:00   69.847159\n",
       "2019-07-01 00:00:00   80.098251\n",
       "2019-08-01 00:00:00   92.792788\n",
       "2019-09-01 00:00:00  108.272558\n",
       "2019-10-01 00:00:00  126.846673\n",
       "2019-11-01 00:00:00  148.734949\n",
       "2019-12-01 00:00:00  173.994782\n",
       "2020-01-01 00:00:00  202.436977\n",
       "2020-02-01 00:00:00  233.542496\n",
       "2020-03-01 00:00:00  266.398756\n",
       "2020-04-01 00:00:00  299.678577\n",
       "2020-05-01 00:00:00  331.683993\n",
       "2020-06-01 00:00:00  360.468434\n",
       "2020-07-01 00:00:00  384.034097\n",
       "2020-08-01 00:00:00  400.579854\n",
       "2020-09-01 00:00:00  408.755527\n",
       "2020-10-01 00:00:00  407.868580\n",
       "2020-11-01 00:00:00  397.994945\n",
       "2020-12-01 00:00:00  379.966718\n",
       "2021-01-01 00:00:00  141.076977\n",
       "2021-02-01 00:00:00  325.673812\n",
       "2021-03-01 00:00:00  293.274932\n",
       "2021-04-01 00:00:00  259.954867\n",
       "2021-05-01 00:00:00  227.346793\n",
       "2021-06-01 00:00:00  196.698725\n",
       "2021-07-01 00:00:00  168.843713\n",
       "2021-08-01 00:00:00  144.231020\n",
       "2021-09-01 00:00:00  122.995327\n",
       "2021-10-01 00:00:00  105.041423\n",
       "2021-11-01 00:00:00   90.126960\n",
       "2021-12-01 00:00:00   77.932622"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy of all tried model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>ModelParameters</th>\n",
       "      <th>TransformationParameters</th>\n",
       "      <th>TransformationRuntime</th>\n",
       "      <th>FitRuntime</th>\n",
       "      <th>PredictRuntime</th>\n",
       "      <th>TotalRuntime</th>\n",
       "      <th>Ensemble</th>\n",
       "      <th>Exceptions</th>\n",
       "      <th>...</th>\n",
       "      <th>spl</th>\n",
       "      <th>contour</th>\n",
       "      <th>smape_weighted</th>\n",
       "      <th>mae_weighted</th>\n",
       "      <th>rmse_weighted</th>\n",
       "      <th>containment_weighted</th>\n",
       "      <th>spl_weighted</th>\n",
       "      <th>contour_weighted</th>\n",
       "      <th>TotalRuntimeSeconds</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d44db758a910a281253b92e582252ac2</td>\n",
       "      <td>UnobservedComponents</td>\n",
       "      <td>{\"level\": true, \"trend\": true, \"cycle\": true, ...</td>\n",
       "      <td>{\"outlier_method\": \"clip\", \"outlier_threshold\"...</td>\n",
       "      <td>00:00:00.070931</td>\n",
       "      <td>00:00:00.000479</td>\n",
       "      <td>00:00:00.363803</td>\n",
       "      <td>00:00:00.435213</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>3.173126</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>95.497762</td>\n",
       "      <td>259.466508</td>\n",
       "      <td>471.664969</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.173126</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>1</td>\n",
       "      <td>20.967692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cca9561b89203c48870ff83ead891601</td>\n",
       "      <td>UnobservedComponents</td>\n",
       "      <td>{\"level\": true, \"trend\": false, \"cycle\": false...</td>\n",
       "      <td>{\"outlier_method\": \"clip\", \"outlier_threshold\"...</td>\n",
       "      <td>00:00:00.072176</td>\n",
       "      <td>00:00:00.000454</td>\n",
       "      <td>00:00:00.041109</td>\n",
       "      <td>00:00:00.113739</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.773188</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>97.991897</td>\n",
       "      <td>260.305556</td>\n",
       "      <td>457.412438</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>2.773188</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>1</td>\n",
       "      <td>20.966305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2aa54e7c13fe40f66c13b8260c1280a5</td>\n",
       "      <td>ETS</td>\n",
       "      <td>{\"damped\": false, \"trend\": \"additive\", \"season...</td>\n",
       "      <td>{\"outlier_method\": \"remove\", \"outlier_threshol...</td>\n",
       "      <td>00:00:00.329598</td>\n",
       "      <td>00:00:00.000462</td>\n",
       "      <td>00:00:00.421009</td>\n",
       "      <td>00:00:00.751069</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>84415.146505</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>102.380037</td>\n",
       "      <td>764.278469</td>\n",
       "      <td>1738.211371</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>84415.146505</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>1</td>\n",
       "      <td>30604.545716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6f5a7c1331153efa322dd9e7238909d2</td>\n",
       "      <td>VECM</td>\n",
       "      <td>{\"deterministic\": \"ci\", \"k_ar_diff\": 1, \"regre...</td>\n",
       "      <td>{\"outlier_method\": \"remove\", \"outlier_threshol...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>TypeError(\"fit() missing 1 required positional...</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28b9b3f150a87a0df0a79396164f243a</td>\n",
       "      <td>VECM</td>\n",
       "      <td>{\"deterministic\": \"colo\", \"k_ar_diff\": 1, \"reg...</td>\n",
       "      <td>{\"outlier_method\": null, \"outlier_threshold\": ...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>ValueError('Only gave one variable to VECM',)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>342</th>\n",
       "      <td>cb3c41ff0846aee799ae75a9bfc8180e</td>\n",
       "      <td>GluonTS</td>\n",
       "      <td>{\"gluon_model\": \"DeepAR\", \"epochs\": 40, \"learn...</td>\n",
       "      <td>{\"outlier_method\": null, \"outlier_threshold\": ...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NameError(\"name 'ListDataset' is not defined\",)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>343</th>\n",
       "      <td>b799c12049c561a45d8b0a7541e805fa</td>\n",
       "      <td>GluonTS</td>\n",
       "      <td>{\"gluon_model\": \"MQCNN\", \"epochs\": 40, \"learni...</td>\n",
       "      <td>{\"outlier_method\": \"clip\", \"outlier_threshold\"...</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>NameError(\"name 'ListDataset' is not defined\",)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>344</th>\n",
       "      <td>bb2691e5c0fb37d30e645ca7bbed96bb</td>\n",
       "      <td>Ensemble</td>\n",
       "      <td>{\"model_name\": \"BestN\", \"model_count\": 3, \"mod...</td>\n",
       "      <td>{}</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:10.222302</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:10.222302</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.290402</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>85.897710</td>\n",
       "      <td>237.254134</td>\n",
       "      <td>409.164199</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>2.290402</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>11</td>\n",
       "      <td>18.116213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>345</th>\n",
       "      <td>5a5bd3468974da504c45ab807c365716</td>\n",
       "      <td>Ensemble</td>\n",
       "      <td>{\"model_name\": \"BestN\", \"model_count\": 3, \"mod...</td>\n",
       "      <td>{}</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:09.649799</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:09.649799</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.256573</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>84.996590</td>\n",
       "      <td>236.532150</td>\n",
       "      <td>392.184857</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>2.256573</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>10</td>\n",
       "      <td>18.076430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>346</th>\n",
       "      <td>6392a8d3d3bb812253f783d5f5953950</td>\n",
       "      <td>Ensemble</td>\n",
       "      <td>{\"model_name\": \"BestN\", \"model_count\": 3, \"mod...</td>\n",
       "      <td>{}</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:10.308530</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>00:00:10.308530</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>2.290402</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>85.897710</td>\n",
       "      <td>237.254134</td>\n",
       "      <td>409.164199</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>2.290402</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>11</td>\n",
       "      <td>18.116213</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>346 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ID                 Model  \\\n",
       "0    d44db758a910a281253b92e582252ac2  UnobservedComponents   \n",
       "1    cca9561b89203c48870ff83ead891601  UnobservedComponents   \n",
       "2    2aa54e7c13fe40f66c13b8260c1280a5                   ETS   \n",
       "3    6f5a7c1331153efa322dd9e7238909d2                  VECM   \n",
       "4    28b9b3f150a87a0df0a79396164f243a                  VECM   \n",
       "..                                ...                   ...   \n",
       "342  cb3c41ff0846aee799ae75a9bfc8180e               GluonTS   \n",
       "343  b799c12049c561a45d8b0a7541e805fa               GluonTS   \n",
       "344  bb2691e5c0fb37d30e645ca7bbed96bb              Ensemble   \n",
       "345  5a5bd3468974da504c45ab807c365716              Ensemble   \n",
       "346  6392a8d3d3bb812253f783d5f5953950              Ensemble   \n",
       "\n",
       "                                       ModelParameters  \\\n",
       "0    {\"level\": true, \"trend\": true, \"cycle\": true, ...   \n",
       "1    {\"level\": true, \"trend\": false, \"cycle\": false...   \n",
       "2    {\"damped\": false, \"trend\": \"additive\", \"season...   \n",
       "3    {\"deterministic\": \"ci\", \"k_ar_diff\": 1, \"regre...   \n",
       "4    {\"deterministic\": \"colo\", \"k_ar_diff\": 1, \"reg...   \n",
       "..                                                 ...   \n",
       "342  {\"gluon_model\": \"DeepAR\", \"epochs\": 40, \"learn...   \n",
       "343  {\"gluon_model\": \"MQCNN\", \"epochs\": 40, \"learni...   \n",
       "344  {\"model_name\": \"BestN\", \"model_count\": 3, \"mod...   \n",
       "345  {\"model_name\": \"BestN\", \"model_count\": 3, \"mod...   \n",
       "346  {\"model_name\": \"BestN\", \"model_count\": 3, \"mod...   \n",
       "\n",
       "                              TransformationParameters TransformationRuntime  \\\n",
       "0    {\"outlier_method\": \"clip\", \"outlier_threshold\"...       00:00:00.070931   \n",
       "1    {\"outlier_method\": \"clip\", \"outlier_threshold\"...       00:00:00.072176   \n",
       "2    {\"outlier_method\": \"remove\", \"outlier_threshol...       00:00:00.329598   \n",
       "3    {\"outlier_method\": \"remove\", \"outlier_threshol...              00:00:00   \n",
       "4    {\"outlier_method\": null, \"outlier_threshold\": ...              00:00:00   \n",
       "..                                                 ...                   ...   \n",
       "342  {\"outlier_method\": null, \"outlier_threshold\": ...              00:00:00   \n",
       "343  {\"outlier_method\": \"clip\", \"outlier_threshold\"...              00:00:00   \n",
       "344                                                 {}              00:00:00   \n",
       "345                                                 {}              00:00:00   \n",
       "346                                                 {}              00:00:00   \n",
       "\n",
       "         FitRuntime  PredictRuntime    TotalRuntime  Ensemble  \\\n",
       "0   00:00:00.000479 00:00:00.363803 00:00:00.435213         0   \n",
       "1   00:00:00.000454 00:00:00.041109 00:00:00.113739         0   \n",
       "2   00:00:00.000462 00:00:00.421009 00:00:00.751069         0   \n",
       "3          00:00:00        00:00:00        00:00:00         0   \n",
       "4          00:00:00        00:00:00        00:00:00         0   \n",
       "..              ...             ...             ...       ...   \n",
       "342        00:00:00        00:00:00        00:00:00         0   \n",
       "343        00:00:00        00:00:00        00:00:00         0   \n",
       "344 00:00:10.222302        00:00:00 00:00:10.222302         1   \n",
       "345 00:00:09.649799        00:00:00 00:00:09.649799         1   \n",
       "346 00:00:10.308530        00:00:00 00:00:10.308530         1   \n",
       "\n",
       "                                            Exceptions  ...           spl  \\\n",
       "0                                                  NaN  ...      3.173126   \n",
       "1                                                  NaN  ...      2.773188   \n",
       "2                                                  NaN  ...  84415.146505   \n",
       "3    TypeError(\"fit() missing 1 required positional...  ...           NaN   \n",
       "4        ValueError('Only gave one variable to VECM',)  ...           NaN   \n",
       "..                                                 ...  ...           ...   \n",
       "342    NameError(\"name 'ListDataset' is not defined\",)  ...           NaN   \n",
       "343    NameError(\"name 'ListDataset' is not defined\",)  ...           NaN   \n",
       "344                                                NaN  ...      2.290402   \n",
       "345                                                NaN  ...      2.256573   \n",
       "346                                                NaN  ...      2.290402   \n",
       "\n",
       "      contour  smape_weighted  mae_weighted  rmse_weighted  \\\n",
       "0    0.514286       95.497762    259.466508     471.664969   \n",
       "1    0.514286       97.991897    260.305556     457.412438   \n",
       "2    0.514286      102.380037    764.278469    1738.211371   \n",
       "3         NaN             NaN           NaN            NaN   \n",
       "4         NaN             NaN           NaN            NaN   \n",
       "..        ...             ...           ...            ...   \n",
       "342       NaN             NaN           NaN            NaN   \n",
       "343       NaN             NaN           NaN            NaN   \n",
       "344  0.485714       85.897710    237.254134     409.164199   \n",
       "345  0.428571       84.996590    236.532150     392.184857   \n",
       "346  0.485714       85.897710    237.254134     409.164199   \n",
       "\n",
       "     containment_weighted  spl_weighted  contour_weighted  \\\n",
       "0                0.333333      3.173126          0.514286   \n",
       "1                0.361111      2.773188          0.514286   \n",
       "2                0.111111  84415.146505          0.514286   \n",
       "3                     NaN           NaN               NaN   \n",
       "4                     NaN           NaN               NaN   \n",
       "..                    ...           ...               ...   \n",
       "342                   NaN           NaN               NaN   \n",
       "343                   NaN           NaN               NaN   \n",
       "344              0.916667      2.290402          0.485714   \n",
       "345              0.611111      2.256573          0.428571   \n",
       "346              0.916667      2.290402          0.485714   \n",
       "\n",
       "     TotalRuntimeSeconds         Score  \n",
       "0                      1     20.967692  \n",
       "1                      1     20.966305  \n",
       "2                      1  30604.545716  \n",
       "3                      1           NaN  \n",
       "4                      1           NaN  \n",
       "..                   ...           ...  \n",
       "342                    1           NaN  \n",
       "343                    1           NaN  \n",
       "344                   11     18.116213  \n",
       "345                   10     18.076430  \n",
       "346                   11     18.116213  \n",
       "\n",
       "[346 rows x 26 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aggregated from cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Model</th>\n",
       "      <th>ModelParameters</th>\n",
       "      <th>TransformationParameters</th>\n",
       "      <th>Ensemble</th>\n",
       "      <th>Runs</th>\n",
       "      <th>smape</th>\n",
       "      <th>mae</th>\n",
       "      <th>rmse</th>\n",
       "      <th>containment</th>\n",
       "      <th>spl</th>\n",
       "      <th>contour</th>\n",
       "      <th>smape_weighted</th>\n",
       "      <th>mae_weighted</th>\n",
       "      <th>rmse_weighted</th>\n",
       "      <th>containment_weighted</th>\n",
       "      <th>contour_weighted</th>\n",
       "      <th>spl_weighted</th>\n",
       "      <th>TotalRuntimeSeconds</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0251946ab42bfaacd9bd3de8837557df</td>\n",
       "      <td>UnobservedComponents</td>\n",
       "      <td>{\"level\": false, \"trend\": false, \"cycle\": true...</td>\n",
       "      <td>{\"outlier_method\": \"clip\", \"outlier_threshold\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>174.164291</td>\n",
       "      <td>824.851949</td>\n",
       "      <td>996.217541</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>5.393087</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>174.164291</td>\n",
       "      <td>824.851949</td>\n",
       "      <td>996.217541</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>5.393087</td>\n",
       "      <td>1</td>\n",
       "      <td>39.514396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0369b5d4e518d323bed167cbc44e79cf</td>\n",
       "      <td>ETS</td>\n",
       "      <td>{\"damped\": true, \"trend\": \"additive\", \"seasona...</td>\n",
       "      <td>{\"outlier_method\": null, \"outlier_threshold\": ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>93.560451</td>\n",
       "      <td>279.300716</td>\n",
       "      <td>436.379157</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>2.578733</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>93.560451</td>\n",
       "      <td>279.300716</td>\n",
       "      <td>436.379157</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>2.578733</td>\n",
       "      <td>1</td>\n",
       "      <td>19.649026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0437ee871c01346019c91fdd22f1b4f5</td>\n",
       "      <td>FBProphet</td>\n",
       "      <td>{\"holiday\": false, \"regression_type\": null}</td>\n",
       "      <td>{\"outlier_method\": null, \"outlier_threshold\": ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>126.500043</td>\n",
       "      <td>299.424131</td>\n",
       "      <td>514.514590</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>4.657249</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>126.500043</td>\n",
       "      <td>299.424131</td>\n",
       "      <td>514.514590</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>4.657249</td>\n",
       "      <td>4</td>\n",
       "      <td>26.285515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>04ca4d4115aa7487ac0c0b115fa8e6b4</td>\n",
       "      <td>AverageValueNaive</td>\n",
       "      <td>{\"method\": \"Mean\"}</td>\n",
       "      <td>{\"outlier_method\": \"clip\", \"outlier_threshold\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>101.391478</td>\n",
       "      <td>264.514883</td>\n",
       "      <td>461.768166</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>3.300234</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>101.391478</td>\n",
       "      <td>264.514883</td>\n",
       "      <td>461.768166</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.314286</td>\n",
       "      <td>3.300234</td>\n",
       "      <td>1</td>\n",
       "      <td>21.572409</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>061331829ab67164a5a40de66c2526ca</td>\n",
       "      <td>FBProphet</td>\n",
       "      <td>{\"holiday\": true, \"regression_type\": null}</td>\n",
       "      <td>{\"outlier_method\": \"clip\", \"outlier_threshold\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>157.978927</td>\n",
       "      <td>104166.962050</td>\n",
       "      <td>608887.714571</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>227020.398267</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>157.978927</td>\n",
       "      <td>104166.962050</td>\n",
       "      <td>608887.714571</td>\n",
       "      <td>0.138889</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>227020.398267</td>\n",
       "      <td>4</td>\n",
       "      <td>89114.015619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>fb886a4dd28773eb301cdaa9810b2fdf</td>\n",
       "      <td>LastValueNaive</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"outlier_method\": null, \"outlier_threshold\": ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>97.991897</td>\n",
       "      <td>260.305556</td>\n",
       "      <td>457.412438</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>2.913181</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>97.991897</td>\n",
       "      <td>260.305556</td>\n",
       "      <td>457.412438</td>\n",
       "      <td>0.305556</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>2.913181</td>\n",
       "      <td>1</td>\n",
       "      <td>21.072554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>fd07d24498ee9e6b91c5a1be438310f1</td>\n",
       "      <td>SeasonalNaive</td>\n",
       "      <td>{\"method\": \"Mean\", \"lag_1\": 28, \"lag_2\": \"None\"}</td>\n",
       "      <td>{\"outlier_method\": \"clip\", \"outlier_threshold\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>118.647569</td>\n",
       "      <td>340.843855</td>\n",
       "      <td>487.785817</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>3.574598</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>118.647569</td>\n",
       "      <td>340.843855</td>\n",
       "      <td>487.785817</td>\n",
       "      <td>0.361111</td>\n",
       "      <td>0.457143</td>\n",
       "      <td>3.574598</td>\n",
       "      <td>1</td>\n",
       "      <td>24.528099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>fe7bc4d124366b4db59ea591dc96eb38</td>\n",
       "      <td>ZeroesNaive</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"outlier_method\": \"clip\", \"outlier_threshold\"...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>111.659591</td>\n",
       "      <td>284.138889</td>\n",
       "      <td>497.555162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.479667</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>111.659591</td>\n",
       "      <td>284.138889</td>\n",
       "      <td>497.555162</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>4.479667</td>\n",
       "      <td>1</td>\n",
       "      <td>24.216810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>fefc9b5c7db8efd25c918d49b13ffbbf</td>\n",
       "      <td>GLS</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"outlier_method\": \"remove\", \"outlier_threshol...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>97.385798</td>\n",
       "      <td>259.510660</td>\n",
       "      <td>455.411313</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>2.270222</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>97.385798</td>\n",
       "      <td>259.510660</td>\n",
       "      <td>455.411313</td>\n",
       "      <td>0.527778</td>\n",
       "      <td>0.485714</td>\n",
       "      <td>2.270222</td>\n",
       "      <td>1</td>\n",
       "      <td>20.515372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>ff662c063094a17920f4e9442d1f691c</td>\n",
       "      <td>GLS</td>\n",
       "      <td>{}</td>\n",
       "      <td>{\"outlier_method\": \"remove\", \"outlier_threshol...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>94.440897</td>\n",
       "      <td>365.412847</td>\n",
       "      <td>486.929327</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>2.499027</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>94.440897</td>\n",
       "      <td>365.412847</td>\n",
       "      <td>486.929327</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.514286</td>\n",
       "      <td>2.499027</td>\n",
       "      <td>1</td>\n",
       "      <td>20.581746</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   ID                 Model  \\\n",
       "0    0251946ab42bfaacd9bd3de8837557df  UnobservedComponents   \n",
       "1    0369b5d4e518d323bed167cbc44e79cf                   ETS   \n",
       "2    0437ee871c01346019c91fdd22f1b4f5             FBProphet   \n",
       "3    04ca4d4115aa7487ac0c0b115fa8e6b4     AverageValueNaive   \n",
       "4    061331829ab67164a5a40de66c2526ca             FBProphet   \n",
       "..                                ...                   ...   \n",
       "230  fb886a4dd28773eb301cdaa9810b2fdf        LastValueNaive   \n",
       "231  fd07d24498ee9e6b91c5a1be438310f1         SeasonalNaive   \n",
       "232  fe7bc4d124366b4db59ea591dc96eb38           ZeroesNaive   \n",
       "233  fefc9b5c7db8efd25c918d49b13ffbbf                   GLS   \n",
       "234  ff662c063094a17920f4e9442d1f691c                   GLS   \n",
       "\n",
       "                                       ModelParameters  \\\n",
       "0    {\"level\": false, \"trend\": false, \"cycle\": true...   \n",
       "1    {\"damped\": true, \"trend\": \"additive\", \"seasona...   \n",
       "2          {\"holiday\": false, \"regression_type\": null}   \n",
       "3                                   {\"method\": \"Mean\"}   \n",
       "4           {\"holiday\": true, \"regression_type\": null}   \n",
       "..                                                 ...   \n",
       "230                                                 {}   \n",
       "231   {\"method\": \"Mean\", \"lag_1\": 28, \"lag_2\": \"None\"}   \n",
       "232                                                 {}   \n",
       "233                                                 {}   \n",
       "234                                                 {}   \n",
       "\n",
       "                              TransformationParameters  Ensemble  Runs  \\\n",
       "0    {\"outlier_method\": \"clip\", \"outlier_threshold\"...         0     1   \n",
       "1    {\"outlier_method\": null, \"outlier_threshold\": ...         0     1   \n",
       "2    {\"outlier_method\": null, \"outlier_threshold\": ...         0     1   \n",
       "3    {\"outlier_method\": \"clip\", \"outlier_threshold\"...         0     1   \n",
       "4    {\"outlier_method\": \"clip\", \"outlier_threshold\"...         0     1   \n",
       "..                                                 ...       ...   ...   \n",
       "230  {\"outlier_method\": null, \"outlier_threshold\": ...         0     1   \n",
       "231  {\"outlier_method\": \"clip\", \"outlier_threshold\"...         0     1   \n",
       "232  {\"outlier_method\": \"clip\", \"outlier_threshold\"...         0     1   \n",
       "233  {\"outlier_method\": \"remove\", \"outlier_threshol...         0     1   \n",
       "234  {\"outlier_method\": \"remove\", \"outlier_threshol...         0     1   \n",
       "\n",
       "          smape            mae           rmse  containment            spl  \\\n",
       "0    174.164291     824.851949     996.217541     0.361111       5.393087   \n",
       "1     93.560451     279.300716     436.379157     0.916667       2.578733   \n",
       "2    126.500043     299.424131     514.514590     0.138889       4.657249   \n",
       "3    101.391478     264.514883     461.768166     0.444444       3.300234   \n",
       "4    157.978927  104166.962050  608887.714571     0.138889  227020.398267   \n",
       "..          ...            ...            ...          ...            ...   \n",
       "230   97.991897     260.305556     457.412438     0.305556       2.913181   \n",
       "231  118.647569     340.843855     487.785817     0.361111       3.574598   \n",
       "232  111.659591     284.138889     497.555162     0.000000       4.479667   \n",
       "233   97.385798     259.510660     455.411313     0.527778       2.270222   \n",
       "234   94.440897     365.412847     486.929327     0.888889       2.499027   \n",
       "\n",
       "      contour  smape_weighted   mae_weighted  rmse_weighted  \\\n",
       "0    0.514286      174.164291     824.851949     996.217541   \n",
       "1    0.485714       93.560451     279.300716     436.379157   \n",
       "2    0.428571      126.500043     299.424131     514.514590   \n",
       "3    0.314286      101.391478     264.514883     461.768166   \n",
       "4    0.514286      157.978927  104166.962050  608887.714571   \n",
       "..        ...             ...            ...            ...   \n",
       "230  0.514286       97.991897     260.305556     457.412438   \n",
       "231  0.457143      118.647569     340.843855     487.785817   \n",
       "232  0.514286      111.659591     284.138889     497.555162   \n",
       "233  0.485714       97.385798     259.510660     455.411313   \n",
       "234  0.514286       94.440897     365.412847     486.929327   \n",
       "\n",
       "     containment_weighted  contour_weighted   spl_weighted  \\\n",
       "0                0.361111          0.514286       5.393087   \n",
       "1                0.916667          0.485714       2.578733   \n",
       "2                0.138889          0.428571       4.657249   \n",
       "3                0.444444          0.314286       3.300234   \n",
       "4                0.138889          0.514286  227020.398267   \n",
       "..                    ...               ...            ...   \n",
       "230              0.305556          0.514286       2.913181   \n",
       "231              0.361111          0.457143       3.574598   \n",
       "232              0.000000          0.514286       4.479667   \n",
       "233              0.527778          0.485714       2.270222   \n",
       "234              0.888889          0.514286       2.499027   \n",
       "\n",
       "     TotalRuntimeSeconds         Score  \n",
       "0                      1     39.514396  \n",
       "1                      1     19.649026  \n",
       "2                      4     26.285515  \n",
       "3                      1     21.572409  \n",
       "4                      4  89114.015619  \n",
       "..                   ...           ...  \n",
       "230                    1     21.072554  \n",
       "231                    1     24.528099  \n",
       "232                    1     24.216810  \n",
       "233                    1     20.515372  \n",
       "234                    1     20.581746  \n",
       "\n",
       "[235 rows x 20 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
